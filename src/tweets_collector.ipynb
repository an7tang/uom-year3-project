{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Tweets Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, json, time\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------ Helper function for creating timestamp ------------\n",
    "def today(backward_days: int):\n",
    "    d = datetime.utcnow() - timedelta(days=backward_days)\n",
    "    date = datetime(year=d.year, month=d.month, day=d.day, hour=0, minute=0, second=0)\n",
    "    return date\n",
    "\n",
    "def file_timestamp(datetime):\n",
    "    return f\"{datetime.strftime('%y')}{datetime.month:02}{datetime.day:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TweetsCollector:\n",
    "\n",
    "    bearer_token = os.getenv('TWITTER_BEARER_TOKEN')\n",
    "    tweet_fields = ['created_at']\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = tweepy.Client(self.bearer_token, wait_on_rate_limit=True)\n",
    "\n",
    "    def _anonymise_data(self, content: str) -> str:\n",
    "        # pattern_username = r\"(?<![\\w@!#$%&*])(@\\w{1,15})\\b\"  # Match '@username'\n",
    "        # pattern_url = r\"(?:https://|http://)[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9\\(\\)]{1,6}\\b[-a-zA-Z0-9\\(\\)@:%_\\+.~#?&//=]*\"\n",
    "        \n",
    "        # usernames = re.findall(pattern_username, content)\n",
    "        # for i, name in enumerate(usernames):\n",
    "        #     alias = f'USERNAME_{(i+1):02}'\n",
    "        #     content = re.sub(name, alias, content)\n",
    "        \n",
    "        # urls = re.findall(pattern_url, content)\n",
    "        # for i, url in enumerate(urls):\n",
    "        #     alias = f'URL_{(i+1):02}'\n",
    "        #     content = re.sub(url, alias, content)\n",
    "        \n",
    "        # return content\n",
    "        ...\n",
    "    \n",
    "    def anonymise_tweets_list(self, tweets_list: str) -> str:\n",
    "        pattern_username = r\"(?<![\\w@!#$%&*])(@\\w{1,15})\\b\"  # Match '@username'\n",
    "        pattern_url = r\"(?:https://|http://)[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9]{1,6}\\b[-a-zA-Z0-9@:%_\\+.~#?&//=]*\"\n",
    "\n",
    "        new_list = []\n",
    "        for index, tweet in enumerate(tweets_list):\n",
    "            content = tweet['content']\n",
    "            usernames = re.findall(pattern_username, content)\n",
    "            \n",
    "            # print(f\"{index}: {content}\", end=\"\\r\")\n",
    "            try:\n",
    "                for i, name in enumerate(usernames):\n",
    "                    alias = f'USERNAME_{(i+1):02}'\n",
    "                    content = re.sub(name, alias, content)\n",
    "                \n",
    "                urls = re.findall(pattern_url, content)\n",
    "                for i, url in enumerate(urls):\n",
    "                    alias = f'URL_{(i+1):02}'\n",
    "                    content = re.sub(url, alias, content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurs in: index [{index}] :\\n{content}\", end=\"\\r\")\n",
    "                raise\n",
    "\n",
    "\n",
    "            new_tweet = {\n",
    "                \"creation_date\": tweet[\"creation_date\"],\n",
    "                \"content\": content\n",
    "            }\n",
    "\n",
    "            new_list.append(new_tweet)\n",
    "        \n",
    "        return new_list\n",
    "\n",
    "    def count_tweet(self, query, mute=False):\n",
    "        counts = self.client.get_recent_tweets_count(query=query, granularity='day')\n",
    "\n",
    "        if mute:\n",
    "            return counts\n",
    "\n",
    "        str_print = ''\n",
    "        total_count = counts.meta['total_tweet_count']\n",
    "\n",
    "        for count in counts.data:\n",
    "            start_time = re.search('\\d{4}-(\\d{2}-\\d{2})', count['start']).group(1)\n",
    "            end_time = re.search('\\d{4}-(\\d{2}-\\d{2})', count['end']).group(1)\n",
    "            str_print += f\"{start_time} => {end_time} :  {count['tweet_count']}\\n\"\n",
    "        str_print = f\"Average: {total_count/7:.0f}/day\\nTotal : {total_count} in 7 days\\n\\n\" + str_print\n",
    "        print(str_print)\n",
    "\n",
    "        return counts\n",
    "\n",
    "    def limit_handler(self, paginator):\n",
    "        while True:\n",
    "            try:\n",
    "                yield next(paginator)\n",
    "            except tweepy.errors.TooManyRequests:\n",
    "                print('\\nReached rate limite. Sleeping for >15 minutes')\n",
    "                time.sleep(15 * 61)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "    def search_tweets_pagination(self, query: str, num: int, start_date, end_date):\n",
    "        tweets = self.limit_handler(\n",
    "            tweepy.Paginator(\n",
    "                self.client.search_recent_tweets, \n",
    "                query=query, \n",
    "                max_results=100, # max limit: 100\n",
    "                tweet_fields=self.tweet_fields,\n",
    "                start_time=start_date,\n",
    "                end_time=end_date,\n",
    "            ).flatten(limit=num)\n",
    "        )\n",
    "        \n",
    "        return tweets\n",
    "    \n",
    "    def convert_tweets_to_dataframe(self, tweets: tweepy.Response) -> pd.DataFrame:\n",
    "    #     tweets_list = []\n",
    "    #     # with pagination: for tweet in tweets\n",
    "    #     # without pagination: for tweet in tweets.data\n",
    "    #     for tweet in tweets:\n",
    "    #         set_tweet_data= {\n",
    "    #             'created_at': tweet.created_at,\n",
    "    #             'text': self._anonymise_data(tweet.text)\n",
    "    #         }\n",
    "    #         tweets_list.append(set_tweet_data)\n",
    "        \n",
    "    #     df = pd.DataFrame(tweets_list)\n",
    "    #     return df\n",
    "        ...\n",
    "\n",
    "\n",
    "    def convert_tweets_to_list_of_dict(self, tweets: tweepy.Response) -> list:\n",
    "        tweets_list = []\n",
    "        # with pagination:     for tweet in tweets\n",
    "        # without pagination:  for tweet in tweets.data\n",
    "        for i, tweet in enumerate(tweets):\n",
    "            tweet_dict = {\n",
    "                \"creation_date\": tweet.data[\"created_at\"],\n",
    "                \"content\": tweet.data[\"text\"]\n",
    "            }\n",
    "            tweets_list.append(tweet_dict)\n",
    "            print(f'Current number: {i+1}', end='\\r')\n",
    "        print('\\n\\033[32mConvertion successfully finished!\\033[0m')\n",
    "        # tweets_json = json.dumps(tweets_list, indent=2)\n",
    "        return tweets_list\n",
    "\n",
    "collector = TweetsCollector()\n",
    "\n",
    "query = \"(covid OR covid19 OR covid-19 OR coronavirus OR (corona virus) OR pandemic) -is:retweet lang:en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collection date: '2023-03-04T00:00:00Z' -> \u001b[32m2 day(s) ago\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "backward_days = 2   # Set to 1 to collect yesterday's (Max: 6)\n",
    "collection_date = today(backward_days)\n",
    "start_time = collection_date\n",
    "end_time = today(backward_days) + timedelta(days=1)\n",
    "print(f\"The collection date: '{collection_date.strftime('%Y-%m-%dT%H:%M:%SZ')}' -> \\033[32m{backward_days} day(s) ago\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 118869/day\n",
      "Total : 832085 in 7 days\n",
      "\n",
      "02-27 => 02-28 :  70600\n",
      "02-28 => 03-01 :  141809\n",
      "03-01 => 03-02 :  136013\n",
      "03-02 => 03-03 :  120296\n",
      "03-03 => 03-04 :  109771\n",
      "03-04 => 03-05 :  95536\n",
      "03-05 => 03-06 :  105254\n",
      "03-06 => 03-06 :  52806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------- Count tweets --------------\n",
    "tweet_counts = collector.count_tweet(query)\n",
    "\n",
    "counts_in_hour = collector.client.get_recent_tweets_count(query=query, granularity='hour', start_time=start_time, end_time=end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2023-03-04T23:00:00.000Z\n",
      "Total number of tweets: 95535\n",
      "Expected collected number: 47763\n"
     ]
    }
   ],
   "source": [
    "counts = []\n",
    "for data in reversed(counts_in_hour.data):\n",
    "    start = data['start']\n",
    "    end = (datetime.strptime(data['end'], '%Y-%m-%dT%H:%M:%S.000Z') - timedelta(seconds=1)).strftime('%Y-%m-%dT%H:%M:%S.000Z')\n",
    "    counts.append({\n",
    "        \"end\": end,\n",
    "        \"start\": start,\n",
    "        \"tweet_count\": data['tweet_count']\n",
    "    })\n",
    "\n",
    "expected_num = np.sum([hour['tweet_count'] // 2 for hour in counts])\n",
    "print(f\"Date: {counts_in_hour.data[-1]['start']}\\\n",
    "\\nTotal number of tweets: {counts_in_hour.meta['total_tweet_count']}\\\n",
    "\\nExpected collected number: {expected_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 2023-03-04T23:00:00.000Z -----\n",
      "Current number: 2368\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T22:00:00.000Z -----\n",
      "Current number: 2458\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T21:00:00.000Z -----\n",
      "Current number: 2339\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T20:00:00.000Z -----\n",
      "Current number: 2322\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T19:00:00.000Z -----\n",
      "Current number: 2369\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T18:00:00.000Z -----\n",
      "Current number: 2448\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T17:00:00.000Z -----\n",
      "Current number: 2601\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T16:00:00.000Z -----\n",
      "Current number: 2728\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T15:00:00.000Z -----\n",
      "Current number: 2628\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T14:00:00.000Z -----\n",
      "Current number: 2444\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T13:00:00.000Z -----\n",
      "Current number: 2238\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T12:00:00.000Z -----\n",
      "Current number: 1891\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T11:00:00.000Z -----\n",
      "Current number: 599\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 634 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number: 1574\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T10:00:00.000Z -----\n",
      "Current number: 1494\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T09:00:00.000Z -----\n",
      "Current number: 1479\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T08:00:00.000Z -----\n",
      "Current number: 1559\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T07:00:00.000Z -----\n",
      "Current number: 1361\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T06:00:00.000Z -----\n",
      "Current number: 1309\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T05:00:00.000Z -----\n",
      "Current number: 1330\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T04:00:00.000Z -----\n",
      "Current number: 1541\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T03:00:00.000Z -----\n",
      "Current number: 1634\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T02:00:00.000Z -----\n",
      "Current number: 1806\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T01:00:00.000Z -----\n",
      "Current number: 1805\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-04T00:00:00.000Z -----\n",
      "Current number: 2037\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tweets_list = []\n",
    "for hour in counts:\n",
    "    start = hour['start']\n",
    "    end = hour['end']\n",
    "    num = hour['tweet_count'] // 2  # Get 50%\n",
    "    print(f\"----- {start} -----\")\n",
    "    generator = collector.search_tweets_pagination(query, num, start, end)\n",
    "    tweets = collector.convert_tweets_to_list_of_dict(generator)\n",
    "    tweets_list.extend(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anonymous_tweets_list = collector.anonymise_tweets_list(tweets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'tweets_230304_#47763.json' is created.\n"
     ]
    }
   ],
   "source": [
    "# path = './data/raw/'\n",
    "path = \"/home/p11333at/nlp-project/data/raw/\"\n",
    "filename = f\"tweets_{file_timestamp(start_time)}_#{len(tweets_list)}.json\"\n",
    "\n",
    "with open(f\"{path}{filename}\", \"w\") as f:\n",
    "    for line in anonymous_tweets_list:\n",
    "        json.dump(line, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "if os.path.exists(f\"{path}{filename}\"):\n",
    "    print(f\"The file '{filename}' is created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58456bdcf71a0aafcc61ee26750cc6fcc46b3f534d6a5983f0e2590bcc774fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
