{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Tweets Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, re, json, time\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ------------ Helper function for creating timestamp ------------\n",
    "def today(backward_days: int):\n",
    "    d = datetime.utcnow() - timedelta(days=backward_days)\n",
    "    date = datetime(year=d.year, month=d.month, day=d.day, hour=0, minute=0, second=0)\n",
    "    return date\n",
    "\n",
    "def file_timestamp(datetime):\n",
    "    return f\"{datetime.strftime('%y')}{datetime.month:02}{datetime.day:02}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TweetsCollector:\n",
    "\n",
    "    # bearer_token = os.getenv('TWITTER_BEARER_TOKEN')\n",
    "    bearer_token = os.getenv('TWITTER_BEARER_TOKEN_2')  # Backup token\n",
    "    tweet_fields = ['created_at']\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = tweepy.Client(self.bearer_token, wait_on_rate_limit=True)\n",
    "\n",
    "    def _anonymise_data(self, content: str) -> str:\n",
    "        # pattern_username = r\"(?<![\\w@!#$%&*])(@\\w{1,15})\\b\"  # Match '@username'\n",
    "        # pattern_url = r\"(?:https://|http://)[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9\\(\\)]{1,6}\\b[-a-zA-Z0-9\\(\\)@:%_\\+.~#?&//=]*\"\n",
    "        \n",
    "        # usernames = re.findall(pattern_username, content)\n",
    "        # for i, name in enumerate(usernames):\n",
    "        #     alias = f'USERNAME_{(i+1):02}'\n",
    "        #     content = re.sub(name, alias, content)\n",
    "        \n",
    "        # urls = re.findall(pattern_url, content)\n",
    "        # for i, url in enumerate(urls):\n",
    "        #     alias = f'URL_{(i+1):02}'\n",
    "        #     content = re.sub(url, alias, content)\n",
    "        \n",
    "        # return content\n",
    "        ...\n",
    "    \n",
    "    def anonymise_tweets_list(self, tweets_list: str) -> str:\n",
    "        pattern_username = r\"(?<![\\w@!#$%&*])(@\\w{1,15})\\b\"  # Match '@username'\n",
    "        pattern_url = r\"(?:https://|http://)[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9]{1,6}\\b[-a-zA-Z0-9@:%_\\+.~#?&//=]*\"\n",
    "\n",
    "        new_list = []\n",
    "        for index, tweet in enumerate(tweets_list):\n",
    "            content = tweet['content']\n",
    "            \n",
    "            # print(f\"{index}: {content}\", end=\"\\r\")\n",
    "            try:\n",
    "                usernames = re.findall(pattern_username, content)\n",
    "                for i, name in enumerate(usernames):\n",
    "                    alias = f'USERNAME_{(i+1):02}'\n",
    "                    content = re.sub(re.escape(name), alias, content)   # Make sure to escape the string before using it in a regular expression\n",
    "                \n",
    "                urls = re.findall(pattern_url, content)\n",
    "                for i, url in enumerate(urls):\n",
    "                    alias = f'URL_{(i+1):02}'\n",
    "                    content = re.sub(re.escape(url), alias, content)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurs in: index [{index}] :\\n{tweet['content']}\", end=\"\\r\")\n",
    "                raise\n",
    "\n",
    "\n",
    "            new_tweet = {\n",
    "                \"creation_date\": tweet[\"creation_date\"],\n",
    "                \"content\": content\n",
    "            }\n",
    "\n",
    "            new_list.append(new_tweet)\n",
    "        \n",
    "        return new_list\n",
    "\n",
    "    def count_tweet(self, query, mute=False):\n",
    "        counts = self.client.get_recent_tweets_count(query=query, granularity='day')\n",
    "\n",
    "        if mute:\n",
    "            return counts\n",
    "\n",
    "        str_print = ''\n",
    "        total_count = counts.meta['total_tweet_count']\n",
    "\n",
    "        for count in counts.data:\n",
    "            start_time = re.search('\\d{4}-(\\d{2}-\\d{2})', count['start']).group(1)\n",
    "            end_time = re.search('\\d{4}-(\\d{2}-\\d{2})', count['end']).group(1)\n",
    "            str_print += f\"{start_time} => {end_time} :  {count['tweet_count']}\\n\"\n",
    "        str_print = f\"Average: {total_count/7:.0f}/day\\nTotal : {total_count} in 7 days\\n\\n\" + str_print\n",
    "        print(str_print)\n",
    "\n",
    "        return counts\n",
    "\n",
    "    def limit_handler(self, paginator):\n",
    "        while True:\n",
    "            try:\n",
    "                yield next(paginator)\n",
    "            except tweepy.errors.TooManyRequests:\n",
    "                print('\\nReached rate limite. Sleeping for >15 minutes')\n",
    "                time.sleep(15 * 61)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "    def search_tweets_pagination(self, query: str, num: int, start_date, end_date):\n",
    "        tweets = self.limit_handler(\n",
    "            tweepy.Paginator(\n",
    "                self.client.search_recent_tweets, \n",
    "                query=query, \n",
    "                max_results=100, # max limit: 100\n",
    "                tweet_fields=self.tweet_fields,\n",
    "                start_time=start_date,\n",
    "                end_time=end_date,\n",
    "            ).flatten(limit=num)\n",
    "        )\n",
    "        \n",
    "        return tweets\n",
    "    \n",
    "    def convert_tweets_to_dataframe(self, tweets: tweepy.Response) -> pd.DataFrame:\n",
    "    #     tweets_list = []\n",
    "    #     # with pagination: for tweet in tweets\n",
    "    #     # without pagination: for tweet in tweets.data\n",
    "    #     for tweet in tweets:\n",
    "    #         set_tweet_data= {\n",
    "    #             'created_at': tweet.created_at,\n",
    "    #             'text': self._anonymise_data(tweet.text)\n",
    "    #         }\n",
    "    #         tweets_list.append(set_tweet_data)\n",
    "        \n",
    "    #     df = pd.DataFrame(tweets_list)\n",
    "    #     return df\n",
    "        ...\n",
    "\n",
    "\n",
    "    def convert_tweets_to_list_of_dict(self, tweets: tweepy.Response) -> list:\n",
    "        tweets_list = []\n",
    "        # with pagination:     for tweet in tweets\n",
    "        # without pagination:  for tweet in tweets.data\n",
    "        for i, tweet in enumerate(tweets):\n",
    "            tweet_dict = {\n",
    "                \"creation_date\": tweet.data[\"created_at\"],\n",
    "                \"content\": tweet.data[\"text\"]\n",
    "            }\n",
    "            tweets_list.append(tweet_dict)\n",
    "            print(f'Current number: {i+1}', end='\\r')\n",
    "        print('\\n\\033[32mConvertion successfully finished!\\033[0m')\n",
    "        # tweets_json = json.dumps(tweets_list, indent=2)\n",
    "        return tweets_list\n",
    "\n",
    "collector = TweetsCollector()\n",
    "\n",
    "query = \"(covid OR covid19 OR covid-19 OR coronavirus OR (corona virus) OR pandemic) -is:retweet lang:en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collection date: '2023-03-23T00:00:00Z' -> \u001b[32m1 day(s) ago\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "backward_days = 1   # Set to 1 to collect yesterday's (Max: 6)\n",
    "collection_date = today(backward_days)\n",
    "start_time = collection_date\n",
    "end_time = today(backward_days) + timedelta(days=1)\n",
    "print(f\"The collection date: '{collection_date.strftime('%Y-%m-%dT%H:%M:%SZ')}' -> \\033[32m{backward_days} day(s) ago\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 101817/day\n",
      "Total : 712718 in 7 days\n",
      "\n",
      "03-08 => 03-09 :  20835\n",
      "03-09 => 03-10 :  109834\n",
      "03-10 => 03-11 :  105635\n",
      "03-11 => 03-12 :  104148\n",
      "03-12 => 03-13 :  96332\n",
      "03-13 => 03-14 :  97153\n",
      "03-14 => 03-15 :  99752\n",
      "03-15 => 03-15 :  79029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------- Count tweets --------------\n",
    "tweet_counts = collector.count_tweet(query)\n",
    "\n",
    "counts_in_hour = collector.client.get_recent_tweets_count(query=query, granularity='hour', start_time=start_time, end_time=end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2023-03-14T23:00:00.000Z\n",
      "Total number of tweets: 99752\n",
      "Expected collected number: 49871\n"
     ]
    }
   ],
   "source": [
    "counts = []\n",
    "for data in reversed(counts_in_hour.data):\n",
    "    start = data['start']\n",
    "    end = (datetime.strptime(data['end'], '%Y-%m-%dT%H:%M:%S.000Z') - timedelta(seconds=1)).strftime('%Y-%m-%dT%H:%M:%S.000Z')\n",
    "    counts.append({\n",
    "        \"end\": end,\n",
    "        \"start\": start,\n",
    "        \"tweet_count\": data['tweet_count']\n",
    "    })\n",
    "\n",
    "expected_num = np.sum([hour['tweet_count'] // 2 for hour in counts])\n",
    "print(f\"Date: {counts_in_hour.data[-1]['start']}\\\n",
    "\\nTotal number of tweets: {counts_in_hour.meta['total_tweet_count']}\\\n",
    "\\nExpected collected number: {expected_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 2023-03-14T23:00:00.000Z -----\n",
      "Current number: 2102\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T22:00:00.000Z -----\n",
      "Current number: 2308\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T21:00:00.000Z -----\n",
      "Current number: 2392\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T20:00:00.000Z -----\n",
      "Current number: 2766\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T19:00:00.000Z -----\n",
      "Current number: 2575\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T18:00:00.000Z -----\n",
      "Current number: 2563\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T17:00:00.000Z -----\n",
      "Current number: 2719\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T16:00:00.000Z -----\n",
      "Current number: 2778\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T15:00:00.000Z -----\n",
      "Current number: 2728\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T14:00:00.000Z -----\n",
      "Current number: 2787\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T13:00:00.000Z -----\n",
      "Current number: 2608\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T12:00:00.000Z -----\n",
      "Current number: 2224\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T11:00:00.000Z -----\n",
      "Current number: 1879\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T10:00:00.000Z -----\n",
      "Current number: 1506\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T09:00:00.000Z -----\n",
      "Current number: 1367\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T08:00:00.000Z -----\n",
      "Current number: 1413\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T07:00:00.000Z -----\n",
      "Current number: 1312\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T06:00:00.000Z -----\n",
      "Current number: 1103\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T05:00:00.000Z -----\n",
      "Current number: 1328\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T04:00:00.000Z -----\n",
      "Current number: 1539\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T03:00:00.000Z -----\n",
      "Current number: 1383\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 675 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number: 1712\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T02:00:00.000Z -----\n",
      "Current number: 1952\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T01:00:00.000Z -----\n",
      "Current number: 2082\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n",
      "----- 2023-03-14T00:00:00.000Z -----\n",
      "Current number: 2128\n",
      "\u001b[32mConvertion successfully finished!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tweets_list = []\n",
    "for hour in counts:\n",
    "    start = hour['start']\n",
    "    end = hour['end']\n",
    "    num = hour['tweet_count'] // 2  # Get 50%\n",
    "    print(f\"----- {start} -----\")\n",
    "    generator = collector.search_tweets_pagination(query, num, start, end)\n",
    "    tweets = collector.convert_tweets_to_list_of_dict(generator)\n",
    "    tweets_list.extend(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "anonymous_tweets_list = collector.anonymise_tweets_list(tweets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'tweets_230314_#49871.json' is created.\n"
     ]
    }
   ],
   "source": [
    "# path = './data/raw/'\n",
    "path = \"/home/p11333at/nlp-project/data/raw/\"\n",
    "filename = f\"tweets_{file_timestamp(start_time)}_#{len(tweets_list)}.json\"\n",
    "\n",
    "with open(f\"{path}{filename}\", \"w\") as f:\n",
    "    for line in anonymous_tweets_list:\n",
    "        json.dump(line, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "if os.path.exists(f\"{path}{filename}\"):\n",
    "    print(f\"The file '{filename}' is created.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Data Recorder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = TweetsCollector()\n",
    "\n",
    "query = \"(covid OR covid19 OR covid-19 OR coronavirus OR (corona virus) OR pandemic) -is:retweet lang:en\"\n",
    "\n",
    "day_counts = collector.client.get_recent_tweets_count(query=query, granularity='day')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record tweets count for **each day**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2023-03-18': {'total': 91842, 'hourly': {}},\n",
       " '2023-03-19': {'total': 78682, 'hourly': {}},\n",
       " '2023-03-20': {'total': 96013, 'hourly': {}},\n",
       " '2023-03-21': {'total': 99584, 'hourly': {}},\n",
       " '2023-03-22': {'total': 107155, 'hourly': {}},\n",
       " '2023-03-23': {'total': 98178, 'hourly': {}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = {}\n",
    "for i, day in enumerate(day_counts.data):\n",
    "    if i == 0 or i == len(day_counts.data)-1:\n",
    "        continue\n",
    "    date = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", day[\"start\"]).group(0)\n",
    "    count = day[\"tweet_count\"]\n",
    "    output.update(\n",
    "        {date : \n",
    "            {\"total\" : count,\n",
    "             \"hourly\" : {}}\n",
    "        }\n",
    "    )\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record tweets count for **each hour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start time : '2023-03-18T00:00:00Z'\n",
      "The  end  time : '2023-03-24T00:00:00Z'\n"
     ]
    }
   ],
   "source": [
    "# backward_days = 1   # Set to 1 to collect yesterday's (Max: 6) \n",
    "# collection_date = today(backward_days)\n",
    "# start_time = collection_date\n",
    "# end_time = today(backward_days) + timedelta(days=1, seconds=-1)\n",
    "# print(f\"The date: '{collection_date.strftime('%Y-%m-%dT%H:%M:%SZ')}' -> \\033[32m{backward_days} day(s) ago\\033[0m\")\n",
    "\n",
    "start_time = today(6)\n",
    "end_time = today(1) + timedelta(days=1)\n",
    "print(f\"The start time : '{start_time.strftime('%Y-%m-%dT%H:%M:%SZ')}'\")\n",
    "print(f\"The  end  time : '{end_time.strftime('%Y-%m-%dT%H:%M:%SZ')}'\")\n",
    "\n",
    "hour_counts = collector.client.get_recent_tweets_count(query=query, granularity='hour', start_time=start_time, end_time=end_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2023-03-18': {'total': 91842,\n",
       "  'hourly': {'00:00:00': 4450,\n",
       "   '01:00:00': 4059,\n",
       "   '02:00:00': 4065,\n",
       "   '03:00:00': 3646,\n",
       "   '04:00:00': 3285,\n",
       "   '05:00:00': 2659,\n",
       "   '06:00:00': 2497,\n",
       "   '07:00:00': 2584,\n",
       "   '08:00:00': 2556,\n",
       "   '09:00:00': 2567,\n",
       "   '10:00:00': 2834,\n",
       "   '11:00:00': 3494,\n",
       "   '12:00:00': 4371,\n",
       "   '13:00:00': 4543,\n",
       "   '14:00:00': 5138,\n",
       "   '15:00:00': 5278,\n",
       "   '16:00:00': 5338,\n",
       "   '17:00:00': 4700,\n",
       "   '18:00:00': 4645,\n",
       "   '19:00:00': 4307,\n",
       "   '20:00:00': 4035,\n",
       "   '21:00:00': 3728,\n",
       "   '22:00:00': 3682,\n",
       "   '23:00:00': 3380}},\n",
       " '2023-03-19': {'total': 78682,\n",
       "  'hourly': {'00:00:00': 3366,\n",
       "   '01:00:00': 3182,\n",
       "   '02:00:00': 3078,\n",
       "   '03:00:00': 2816,\n",
       "   '04:00:00': 2464,\n",
       "   '05:00:00': 2189,\n",
       "   '06:00:00': 1964,\n",
       "   '07:00:00': 1993,\n",
       "   '08:00:00': 2144,\n",
       "   '09:00:00': 2148,\n",
       "   '10:00:00': 2291,\n",
       "   '11:00:00': 2614,\n",
       "   '12:00:00': 3287,\n",
       "   '13:00:00': 3804,\n",
       "   '14:00:00': 4248,\n",
       "   '15:00:00': 4478,\n",
       "   '16:00:00': 4353,\n",
       "   '17:00:00': 4193,\n",
       "   '18:00:00': 4094,\n",
       "   '19:00:00': 4024,\n",
       "   '20:00:00': 4116,\n",
       "   '21:00:00': 4210,\n",
       "   '22:00:00': 3853,\n",
       "   '23:00:00': 3772}},\n",
       " '2023-03-20': {'total': 96013,\n",
       "  'hourly': {'00:00:00': 3641,\n",
       "   '01:00:00': 3551,\n",
       "   '02:00:00': 3134,\n",
       "   '03:00:00': 2809,\n",
       "   '04:00:00': 2610,\n",
       "   '05:00:00': 2226,\n",
       "   '06:00:00': 2192,\n",
       "   '07:00:00': 2961,\n",
       "   '08:00:00': 2716,\n",
       "   '09:00:00': 2464,\n",
       "   '10:00:00': 2756,\n",
       "   '11:00:00': 3541,\n",
       "   '12:00:00': 4212,\n",
       "   '13:00:00': 4978,\n",
       "   '14:00:00': 5526,\n",
       "   '15:00:00': 5681,\n",
       "   '16:00:00': 5949,\n",
       "   '17:00:00': 5506,\n",
       "   '18:00:00': 5472,\n",
       "   '19:00:00': 5125,\n",
       "   '20:00:00': 4793,\n",
       "   '21:00:00': 4795,\n",
       "   '22:00:00': 4973,\n",
       "   '23:00:00': 4401}},\n",
       " '2023-03-21': {'total': 99584,\n",
       "  'hourly': {'00:00:00': 4629,\n",
       "   '01:00:00': 3930,\n",
       "   '02:00:00': 4033,\n",
       "   '03:00:00': 3247,\n",
       "   '04:00:00': 2698,\n",
       "   '05:00:00': 2278,\n",
       "   '06:00:00': 2512,\n",
       "   '07:00:00': 2542,\n",
       "   '08:00:00': 2844,\n",
       "   '09:00:00': 2690,\n",
       "   '10:00:00': 3277,\n",
       "   '11:00:00': 3500,\n",
       "   '12:00:00': 4603,\n",
       "   '13:00:00': 5170,\n",
       "   '14:00:00': 5764,\n",
       "   '15:00:00': 5378,\n",
       "   '16:00:00': 5324,\n",
       "   '17:00:00': 5294,\n",
       "   '18:00:00': 5690,\n",
       "   '19:00:00': 5267,\n",
       "   '20:00:00': 4933,\n",
       "   '21:00:00': 4978,\n",
       "   '22:00:00': 4501,\n",
       "   '23:00:00': 4500}},\n",
       " '2023-03-22': {'total': 107155,\n",
       "  'hourly': {'00:00:00': 4120,\n",
       "   '01:00:00': 3921,\n",
       "   '02:00:00': 3574,\n",
       "   '03:00:00': 3057,\n",
       "   '04:00:00': 2572,\n",
       "   '05:00:00': 2218,\n",
       "   '06:00:00': 2538,\n",
       "   '07:00:00': 2597,\n",
       "   '08:00:00': 2590,\n",
       "   '09:00:00': 3065,\n",
       "   '10:00:00': 3727,\n",
       "   '11:00:00': 4053,\n",
       "   '12:00:00': 4942,\n",
       "   '13:00:00': 5551,\n",
       "   '14:00:00': 6068,\n",
       "   '15:00:00': 7229,\n",
       "   '16:00:00': 7316,\n",
       "   '17:00:00': 6490,\n",
       "   '18:00:00': 5988,\n",
       "   '19:00:00': 5623,\n",
       "   '20:00:00': 5331,\n",
       "   '21:00:00': 5125,\n",
       "   '22:00:00': 4971,\n",
       "   '23:00:00': 4492}},\n",
       " '2023-03-23': {'total': 98178,\n",
       "  'hourly': {'00:00:00': 4246,\n",
       "   '01:00:00': 3999,\n",
       "   '02:00:00': 3668,\n",
       "   '03:00:00': 3177,\n",
       "   '04:00:00': 2652,\n",
       "   '05:00:00': 2340,\n",
       "   '06:00:00': 2714,\n",
       "   '07:00:00': 3015,\n",
       "   '08:00:00': 3130,\n",
       "   '09:00:00': 3389,\n",
       "   '10:00:00': 3755,\n",
       "   '11:00:00': 4005,\n",
       "   '12:00:00': 4841,\n",
       "   '13:00:00': 5073,\n",
       "   '14:00:00': 4918,\n",
       "   '15:00:00': 5208,\n",
       "   '16:00:00': 5368,\n",
       "   '17:00:00': 5332,\n",
       "   '18:00:00': 5393,\n",
       "   '19:00:00': 4854,\n",
       "   '20:00:00': 4671,\n",
       "   '21:00:00': 4392,\n",
       "   '22:00:00': 4284,\n",
       "   '23:00:00': 3754}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for hour in hour_counts.data:\n",
    "    start = hour[\"start\"]\n",
    "    date = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", start).group(0)\n",
    "    time = re.search(r\"\\d{2}:\\d{2}:\\d{2}\", start).group(0)\n",
    "    # print(time, \":\", hour[\"tweet_count\"])\n",
    "    output[date][\"hourly\"].update({time : hour[\"tweet_count\"]})\n",
    "\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_meta = \"meta.json\"\n",
    "path = \"/home/p11333at/nlp-project/data/\"\n",
    "\n",
    "if os.path.exists(f\"{path}{filename_meta}\"):\n",
    "    with open(f\"{path}{filename_meta}\", \"r+\") as f:\n",
    "        meta = json.load(f)\n",
    "        meta.update(output)\n",
    "\n",
    "        f.seek(0)\n",
    "        json.dump(meta, f, indent=2)\n",
    "        f.truncate()\n",
    "else:\n",
    "    with open(f\"{path}{filename_meta}\", \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"The tweet counts is recorded in '{filename_meta}' file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58456bdcf71a0aafcc61ee26750cc6fcc46b3f534d6a5983f0e2590bcc774fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
