{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"/home/p11333at/nlp-project/script/\")\n",
    "from openai_sentiment import OpenAISentiment\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import spampy\n",
    "\n",
    "\n",
    "filename = \"tweets_test_#100.jsonl\"\n",
    "raw_path = '../data/raw/'\n",
    "processed_path = '../data/processed/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out irrelevant tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: [63481 70991 68084 60474 54884 47763 52763 56311 54374 53808 55898 53839\n",
      " 53074 48168 48605 49871]\n",
      "Sample size: [63 70 68 60 54 47 52 56 54 53 55 53 53 48 48 49]\n",
      "The total number of sample: 883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['../data/raw/tweets_230227_#63481.json',\n",
       "  '../data/raw/tweets_230228_#70991.json',\n",
       "  '../data/raw/tweets_230301_#68084.json',\n",
       "  '../data/raw/tweets_230302_#60474.json',\n",
       "  '../data/raw/tweets_230303_#54884.json',\n",
       "  '../data/raw/tweets_230304_#47763.json',\n",
       "  '../data/raw/tweets_230305_#52763.json',\n",
       "  '../data/raw/tweets_230306_#56311.json',\n",
       "  '../data/raw/tweets_230307_#54374.json',\n",
       "  '../data/raw/tweets_230308_#53808.json',\n",
       "  '../data/raw/tweets_230309_#55898.json',\n",
       "  '../data/raw/tweets_230310_#53839.json',\n",
       "  '../data/raw/tweets_230311_#53074.json',\n",
       "  '../data/raw/tweets_230312_#48168.json',\n",
       "  '../data/raw/tweets_230313_#48605.json',\n",
       "  '../data/raw/tweets_230314_#49871.json'],\n",
       " array([63481, 70991, 68084, 60474, 54884, 47763, 52763, 56311, 54374,\n",
       "        53808, 55898, 53839, 53074, 48168, 48605, 49871]),\n",
       " array([63, 70, 68, 60, 54, 47, 52, 56, 54, 53, 55, 53, 53, 48, 48, 49]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(os.listdir(raw_path))\n",
    "\n",
    "dataset_size = []\n",
    "filepath_list = []\n",
    "for filename in files:\n",
    "    filepath = os.path.join(raw_path, filename)\n",
    "    filepath_list.append(filepath)\n",
    "    with open(filepath) as f:\n",
    "        data = f.readlines()\n",
    "        dataset_size.append(len(data))\n",
    "\n",
    "dataset_size = np.array(dataset_size)\n",
    "\n",
    "sample_size = dataset_size // 1000\n",
    "\n",
    "print(f'Dataset size: {dataset_size}')\n",
    "print(f'Sample size: {sample_size}')\n",
    "print(f'The total number of sample: {sample_size.sum()}')\n",
    "\n",
    "### If you want to skip the some files, use the lines below\n",
    "# filepath_list = filepath_list[10:]\n",
    "# dataset_size = dataset_size[10:]\n",
    "# sample_size = sample_size[10:]\n",
    "filepath_list, dataset_size, sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes = []\n",
    "# for dataset, sample in zip(dataset_size, sample_size):\n",
    "#     np.random.seed(42)\n",
    "#     indexes.append(np.random.choice(dataset, size=sample, replace=False))\n",
    "# indexes = np.array(indexes, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_output = []\n",
    "for i, filepath in enumerate(filepath_list):\n",
    "    with open(filepath, 'r') as f:\n",
    "        # Read as json\n",
    "        data = f.readlines()\n",
    "        data = [json.loads(line) for line in data]\n",
    "\n",
    "        # Get the sample indexes\n",
    "        d_size = dataset_size[i]\n",
    "        s_size = sample_size[i]\n",
    "        np.random.seed(42)  # Set seed for reproducibility\n",
    "        index = np.random.choice(d_size, size=s_size, replace=False)\n",
    "        index = reversed(sorted(index)) # Sort in descending order\n",
    "\n",
    "        # Append the sample to the output\n",
    "        sample_output.extend([data[i] for i in index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data to increase positive samples for balanced dataset\n",
    "\n",
    "# sample_output = []\n",
    "# for i, filepath in enumerate(filepath_list):\n",
    "#     with open(filepath, 'r') as f:\n",
    "#         # Read as json\n",
    "#         data = f.readlines()\n",
    "#         data = [json.loads(line) for line in data]\n",
    "\n",
    "#         # Get the sample indexes\n",
    "#         d_size = dataset_size[i]\n",
    "#         s_size = sample_size[i]\n",
    "#         np.random.seed(42)  # Set seed for reproducibility\n",
    "#         index = np.random.choice(d_size, size=s_size*3, replace=False)[s_size*2:]\n",
    "#         index = reversed(sorted(index)) # Sort in descending order\n",
    "\n",
    "#         # Append the sample to the output\n",
    "#         sample_output.extend([data[i] for i in index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 230227 - 230314\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the date range\n",
    "file_dates = list(map(lambda x: re.search(r\"(?<=\\_)\\d{6}(?=\\_)\", x).group(), filepath_list))\n",
    "start_date, end_date = file_dates[0], file_dates[-1]\n",
    "\n",
    "print(f'Date range: {start_date} - {end_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'positive_2_230227-230314_#883.json' is created.\n"
     ]
    }
   ],
   "source": [
    "# Set the path and filename\n",
    "path = \"/home/p11333at/nlp-project/data/samples/\"\n",
    "filename = f\"samples_{start_date}-{end_date}_#{len(sample_output)}.json\"\n",
    "# filename = f\"positive_2_{start_date}-{end_date}_#{len(sample_output)}.json\"\n",
    "\n",
    "# Write the sample to file\n",
    "with open(f\"{path}{filename}\", \"w\") as f:\n",
    "    for line in sample_output:\n",
    "        json.dump(line, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "if os.path.exists(f\"{path}{filename}\"):\n",
    "    print(f\"The file '{filename}' is created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"USERNAME_01 USERNAME_02 USERNAME_03 Some months back I had no way of taking care of my kids , had no job or money also struggling through the pandemic,so I decided to try Bitcoin trading with the help of USERNAME_04 I was able to earn R130,000 now am financially stable.\"\n",
    "def detect_spam(tweet):\n",
    "    content = tweet['content']\n",
    "    bitcoin_spam = [r\"Some months back I had no way of taking care of my kids\", r\"had no job or money\", r\"try Bitcoin trading\"]\n",
    "    pattern_spam = [bitcoin_spam]\n",
    "    for patterns in pattern_spam:\n",
    "        for p in patterns:\n",
    "            if not re.search(p, content):\n",
    "                break\n",
    "        else:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# detect_spam(test)\n",
    "fname = \"../data/raw/tweets_230227_#63481.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'r') as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(line) for line in data]\n",
    "\n",
    "cleaned_data = [line for line in data if not detect_spam(line)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'w') as f:\n",
    "    for line in cleaned_data:\n",
    "        json.dump(line, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = filter(detect_spam, cleaned_data)\n",
    "spam = list(spam)\n",
    "spam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58456bdcf71a0aafcc61ee26750cc6fcc46b3f534d6a5983f0e2590bcc774fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
