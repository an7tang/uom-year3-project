{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoylxmjPmxBA"
      },
      "source": [
        "# Third Year Project"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h0xv2G9Um3xh"
      },
      "source": [
        "### Download & Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0aH4cRpviLo"
      },
      "outputs": [],
      "source": [
        "# Download libraries unsupported by colab \n",
        "# (Uncomment and run below lines if you use Google Colab)\n",
        "\n",
        "# %pip install contextualSpellCheck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsIvqwn1m0_M",
        "outputId": "ade75b8d-9590-4ed2-d142-d15dfd148599"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anqitang/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to /Users/anqitang/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re, csv, json, datetime\n",
        "import tweepy\n",
        "import nltk, spacy #contextualSpellCheck\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import textdistance\n",
        "# from textblob import TextBlob, Word   # Spell Correction\n",
        "# from spellchecker import SpellChecker\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# --------- Machine Learning ---------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# --------- Machine Learning ---------"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create development dataset (Only run if you don't have dev dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset_path = '/content/drive/MyDrive/Colab Notebooks/year_project/dataset/covid19_tweets.csv'\n",
        "dataset_path = './dataset/covid19_tweets.csv'\n",
        "\n",
        "# Read dataset from a csv file\n",
        "df = pd.read_csv(dataset_path).sample(n=500)   # Get 500 samples\n",
        "new_df = pd.DataFrame()\n",
        "\n",
        "# Save dataframe for development\n",
        "new_df['date'] = df['date']\n",
        "new_df['text'] = df['text']\n",
        "new_df.to_csv('./dataset/dev_dataset.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PGq3oLGSnAbc"
      },
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "-YPYo-3GSb-X"
      },
      "outputs": [],
      "source": [
        "# dataset_path = '/content/drive/MyDrive/Colab Notebooks/year_project/dataset/covid19_tweets.csv'\n",
        "dataset_path = './dataset/dev_dataset.csv'\n",
        "\n",
        "# Read dataset from a csv file\n",
        "df = pd.read_csv(dataset_path).sample(n=500)   # Get 500 samples\n",
        "\n",
        "# Retrieve date and tweet content from DataFrame\n",
        "# pattern_date = r'(\\d{4})-(\\d{2})-(\\d{2})'\n",
        "# raw_dates = df['date'].apply(lambda x: re.match(pattern_date, x).group(0)).to_numpy()\n",
        "raw_dates = df['date'].to_numpy()\n",
        "raw_tweets = df['text'].to_numpy()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "__m5JPQyFRMd"
      },
      "source": [
        "### Split data\n",
        "\n",
        "Split data before pre-processing, to avoid data leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0F0pDgWFYEe",
        "outputId": "0446e3a4-234a-4dbd-de34-406ad8de4474"
      },
      "outputs": [],
      "source": [
        "# Split index for training and valication\n",
        "index = np.arange(len(df))\n",
        "i_train, i_val  = train_test_split(index, test_size=0.2, random_state=1)\n",
        "\n",
        "# Get training data and validation data\n",
        "train_tweets, train_dates = raw_tweets[i_train], raw_dates[i_train]\n",
        "val_tweets, val_dates = raw_tweets[i_val], raw_dates[i_val]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxWmI0XfIcKa"
      },
      "source": [
        "### Spell Correction\n",
        "\n",
        "#### TextBlob library\n",
        "TextBlob(\"sentence ...\").correct()\n",
        "\n",
        "    Time:\n",
        "        100tweets / 51s\n",
        "    Performance:\n",
        "        As tweets are not likely to be 100% grammatically correct,\n",
        "        this library sometimes makes false correction.\n",
        "        For example, \"Trump\" was changed to \"Plump\"\n",
        "    \n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "mm9ETj8UrxOO"
      },
      "outputs": [],
      "source": [
        "class PreProcessor:\n",
        "    '''\n",
        "    Pre-processing tweets:\n",
        "        1) Clean data\n",
        "        2) Tokenisation\n",
        "        3) Spell Correction\n",
        "    '''\n",
        "\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    # Regular Expressions for removals\n",
        "    # re_url = r\"(https://|http://|)[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n",
        "    # re_username = r\"(^|[^@\\w])@(\\w{1,15})\\b\"    # Example: @username\n",
        "    # patterns = f\"{re_url}|{re_username}\"\n",
        "\n",
        "    def _clean_tweet(self, tweet: str) -> str:\n",
        "        # Remove: URL, username\n",
        "        # tweet = re.sub(self.patterns, '', tweet)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        #   1) Replace all kinds of whitespace to exactly one space.\n",
        "        #   2) Remove leading and trailing whitespaces\n",
        "        tweet = re.sub('\\s+', ' ', tweet).strip()\n",
        "\n",
        "        return tweet\n",
        "\n",
        "    def _tokenize(self, tweet: str) -> list:\n",
        "        # contextualSpellCheck.add_to_pipe(nlp)   # Add Spell Checker to Spacy pipeline\n",
        "        \n",
        "        # Tokenisation\n",
        "        doc = self.nlp(tweet)                          # Tokenisation\n",
        "        \n",
        "        # TODO Spell correction\n",
        "        # doc = nlp(doc._.outcome_spellCheck)\n",
        "        \n",
        "        # TODO Remove stopwords\n",
        "\n",
        "        return doc\n",
        "\n",
        "    def _remove_stop_words(self, tokens: list) -> list:\n",
        "        return\n",
        "\n",
        "    def process_tweets(self, tweets: list) -> list:\n",
        "        result = []\n",
        "\n",
        "        for tweet in tweets:\n",
        "\n",
        "            # Remove irrelevant and personal data\n",
        "            cleaned_tweet = self._clean_tweet(tweet)\n",
        "\n",
        "            tokens = self._tokenize(cleaned_tweet)\n",
        "\n",
        "            result.append(tokens)\n",
        "\n",
        "        return np.asarray(result, dtype=object)     # Add \"dtype=object\" to mute a warning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "eTskxUkuVQ65"
      },
      "outputs": [],
      "source": [
        "pp = PreProcessor()\n",
        "\n",
        "pattern_date = r'(\\d{4})-(\\d{2})-(\\d{2})'\n",
        "\n",
        "# Tokenisation\n",
        "train_tokens = pp.process_tweets(train_tweets)\n",
        "# Retrieve date (i.e. remove time)\n",
        "train_dates = np.array([re.match(pattern_date, date).group(0) for date in train_dates])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV5pCPe-k2At",
        "outputId": "1e2bbdb8-b825-4588-dca8-9b6550f28eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hey', 'and', '-', 'would', \"n't\", 'it', 'have', 'made', 'more', 'sense', 'to', 'have', 'the', 'players', 'pay', 'their', 'respects', 'to', 'the', 'A', 'â€¦']\n",
            "<class 'spacy.tokens.doc.Doc'>\n"
          ]
        }
      ],
      "source": [
        "        # /--------------- pySpellChecker 1 ---------------\\\n",
        "        # spell = SpellChecker()\n",
        "        # tokens = [spell.correction(tk) if spell.correction(tk) else tk for tk in tokens]\n",
        "        # /--------------- pySpellChecker 2 ---------------\\\n",
        "        # misspelled = spell.unknown(tokens)\n",
        "        # for typo in misspelled:\n",
        "        #     correction = spell.correction(typo)\n",
        "        #     if not correction:  \n",
        "        #         # If correction is None, the word may be a proper noun rather than misspelled\n",
        "        #         continue\n",
        "        #     try:\n",
        "        #         i = tokens.index(typo)\n",
        "        #         tokens[i] = correction\n",
        "        #     except:\n",
        "        #         print(typo)\n",
        "        #         print(tokens)\n",
        "\n",
        "\n",
        "        # Remove punctuations\n",
        "        # tokens = [re.sub(r'[^A-Za-z0-9]+', '', tk) for tk in tokens]\n",
        "        # Remove empty tokens (Produced when removing punctuations)\n",
        "        # tokens = [tk for tk in tokens if tk]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkGBoAUrRBYd"
      },
      "source": [
        "## Pytorch Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9byklcPzd5bg"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfFBHyJKdFym"
      },
      "outputs": [],
      "source": [
        "# TODO Convert preprocessed data into the x, y below\n",
        "x, y = None, None\n",
        "\n",
        "torch.manual_seed(13)\n",
        "x_tensor = torch.as_tensor(x).float()\n",
        "y_tensor = torch.as_tensor(y).float()\n",
        "# Build dataset containing ALL data points\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "# Performs the split\n",
        "ratio = .8\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * ratio)\n",
        "n_val = n_total - n_train\n",
        "train_data, val_data = random_split(dataset, [n_train, n_val])\n",
        "# Builds a loader of each set\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "val_loader = DataLoader(dataset=val_data, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmLnyH2yd_fI"
      },
      "source": [
        "### Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTiROon5eC-U"
      },
      "outputs": [],
      "source": [
        "# Set learning rate\n",
        "lr = 0.1\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a model\n",
        "# model = nn.Sequential(nn.Linear(1, 1))\n",
        "model = nn.Sequential()\n",
        "model.add_module('linear', nn.Linear(2, 1))\n",
        "\n",
        "# Define a SGD optimizer to update the parameters (now retrieved directly from the model)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "# Define a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGm5c2hARHme"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvDSWt7lYuoJ"
      },
      "outputs": [],
      "source": [
        "class NLPModel(object):\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        # Store the arguments as attributes for later use\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Let's send the model to the specified device right away\n",
        "        self.model.to(self.device)\n",
        "        \n",
        "        # These attributes are defined here, but since they are\n",
        "        # not available at the moment of creation, we keep them None\n",
        "        self.train_loader = None\n",
        "        self.val_loader = None\n",
        "        self.writer = None\n",
        "\n",
        "        # These attributes are going to be computed internally\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.total_epochs = 0\n",
        "\n",
        "        # Creates the train_step function for our model, \n",
        "        # loss function and optimizer\n",
        "        # Note: there are NO ARGS there! It makes use of the class\n",
        "        # attributes directly\n",
        "        self.train_step = self._make_train_step_fn()\n",
        "        # Creates the val_step function for our model and loss\n",
        "        self.val_step = self._make_val_step_fn()\n",
        "\n",
        "    def to(self, device):\n",
        "        # This method allows the user to specify a different device\n",
        "        # It sets the corresponding attribute (to be used later in\n",
        "        # the mini-batches) and sends the model to the device\n",
        "        try:\n",
        "            self.device = device\n",
        "            self.model.to(self.device)\n",
        "        except RuntimeError:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
        "            self.model.to(self.device)\n",
        "\n",
        "    def set_loaders(self, train_loader, val_loader=None):\n",
        "        # This method allows the user to define which train_loader \n",
        "        # (and val_loader, optionally) to use\n",
        "        # Both loaders are then assigned to attributes of the class\n",
        "        # So they can be referred to later\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "    def set_tensorboard(self, name, folder='runs'):\n",
        "        # This method allows the user to create a SummaryWriter to \n",
        "        # interface with TensorBoard\n",
        "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
        "        self.writer = SummaryWriter(f'{folder}/{name}_{suffix}')\n",
        "\n",
        "    def _make_train_step_fn(self):\n",
        "        # This method does not need ARGS... it can refer to\n",
        "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
        "\n",
        "        # Builds function that performs a step in the train loop\n",
        "        def perform_train_step_fn(x, y):\n",
        "            # Sets model to TRAIN mode\n",
        "            self.model.train()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # Step 3 - Computes gradients for both \"b\" and \"w\" parameters\n",
        "            loss.backward()\n",
        "            # Step 4 - Updates parameters using gradients and the\n",
        "            # learning rate\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Returns the loss\n",
        "            return loss.item()\n",
        "\n",
        "        # Returns the function that will be called inside the train loop\n",
        "        return perform_train_step_fn\n",
        "\n",
        "    def _make_val_step_fn(self):\n",
        "        # Builds function that performs a step in the validation loop\n",
        "        def perform_val_step_fn(x, y):\n",
        "            # Sets model to EVAL mode\n",
        "            self.model.eval()\n",
        "\n",
        "            # Step 1 - Computes our model's predicted output - forward pass\n",
        "            yhat = self.model(x)\n",
        "            # Step 2 - Computes the loss\n",
        "            loss = self.loss_fn(yhat, y)\n",
        "            # There is no need to compute Steps 3 and 4, \n",
        "            # since we don't update parameters during evaluation\n",
        "            return loss.item()\n",
        "    \n",
        "    def _mini_batch(self, validation=False):\n",
        "        # The mini-batch can be used with both loaders\n",
        "        # The argument `validation`defines which loader and \n",
        "        # corresponding step function is going to be used\n",
        "        if validation:\n",
        "            data_loader = self.val_loader\n",
        "            step_fn = self.val_step_fn\n",
        "        else:\n",
        "            data_loader = self.train_loader\n",
        "            step_fn = self.train_step_fn\n",
        "\n",
        "        if data_loader is None:\n",
        "            return None\n",
        "\n",
        "        # Once the data loader and step function, this is the same\n",
        "        # mini-batch loop we had before\n",
        "        mini_batch_losses = []\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch = x_batch.to(self.device)\n",
        "            y_batch = y_batch.to(self.device)\n",
        "\n",
        "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
        "            mini_batch_losses.append(mini_batch_loss)\n",
        "\n",
        "        loss = np.mean(mini_batch_losses)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def set_seed(self, seed=42):\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False    \n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    def train(self, n_epochs, seed=42):\n",
        "        # To ensure reproducibility of the training process\n",
        "        self.set_seed(seed)\n",
        "        \n",
        "        for epoch in range(n_epochs):\n",
        "            # Keeps track of the numbers of epochs\n",
        "            # by updating the corresponding attribute\n",
        "            self.total_epochs += 1\n",
        "\n",
        "            # inner loop\n",
        "            # Performs training using mini-batches\n",
        "            loss = self._mini_batch(validation=False)\n",
        "            self.losses.append(loss)\n",
        "\n",
        "            # VALIDATION\n",
        "            # no gradients in validation!\n",
        "            with torch.no_grad():\n",
        "                # Performs evaluation using mini-batches\n",
        "                val_loss = self._mini_batch(validation=True)\n",
        "                self.val_losses.append(val_loss)\n",
        "\n",
        "            # If a SummaryWriter has been set...\n",
        "            if self.writer:\n",
        "                scalars = {'training': loss}\n",
        "                if val_loss is not None:\n",
        "                    scalars.update({'validation': val_loss})\n",
        "                # Records both losses for each epoch under the main tag \"loss\"\n",
        "                self.writer.add_scalars(main_tag='loss',\n",
        "                                        tag_scalar_dict=scalars,\n",
        "                                        global_step=epoch)\n",
        "\n",
        "        if self.writer:\n",
        "            # Flushes the writer\n",
        "            self.writer.flush()\n",
        "\n",
        "    def save_checkpoint(self, filename):\n",
        "        # Builds dictionary with all elements for resuming training\n",
        "        checkpoint = {\n",
        "            'epoch': self.total_epochs,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'loss': self.losses,\n",
        "            'val_loss': self.val_losses\n",
        "        }\n",
        "        torch.save(checkpoint, filename)\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        # Loads dictionary\n",
        "        checkpoint = torch.load(filename)\n",
        "        # Restore state for model and optimizer\n",
        "        self.model.load_state_dict(\n",
        "            checkpoint['model_state_dict']\n",
        "        )\n",
        "        self.optimizer.load_state_dict(\n",
        "            checkpoint['optimizer_state_dict']\n",
        "        )\n",
        "        self.total_epochs = checkpoint['epoch']\n",
        "        self.losses = checkpoint['loss']\n",
        "        self.val_losses = checkpoint['val_loss']\n",
        "        self.model.train() # always use TRAIN for resuming training\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Set it to evaluation mode for predictions\n",
        "        self.model.eval()\n",
        "        # Take a Numpy input and make it a float tensor\n",
        "        x_tensor = torch.as_tensor(x).float()\n",
        "        # Send input to device and use model for prediction\n",
        "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
        "        # Set it back to train mode\n",
        "        self.model.train()\n",
        "        # Detach it, bring it to CPU and back to Numpy\n",
        "        return y_hat_tensor.detach().cpu().numpy()\n",
        "\n",
        "    def plot_losses(self):\n",
        "        fig = plt.figure(figsize=(10, 4))\n",
        "        plt.plot(self.losses, label='Training Loss', c='b')\n",
        "        if self.val_loader:\n",
        "            plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def add_graph(self):\n",
        "        if self.train_loader and self.writer:\n",
        "            # Fetches a single mini-batch so we can use add_graph\n",
        "            x_dummy, y_dummy = next(iter(self.train_loader))\n",
        "            self.writer.add_graph(self.model, x_dummy.to(self.device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAkuedIHfSYS"
      },
      "outputs": [],
      "source": [
        "n_epochs = 200\n",
        "\n",
        "sbs = NLPModel(model, loss_fn, optimizer)\n",
        "sbs.set_loaders(train_loader, val_loader)\n",
        "sbs.set_tensorboard(name='classy')\n",
        "sbs.train(n_epochs=n_epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRWnsXo7j4p7"
      },
      "outputs": [],
      "source": [
        "# One important thing to notice is that the model attribute of the sbs object\n",
        "# is the same object as the model variable created in the model configuration.\n",
        "# It is not a copy!\n",
        "# (Using below code can easily verify this)\n",
        "# print(sbs.model == model)\n",
        "\n",
        "\n",
        "# Make prediction for new, never seen before data points\n",
        "# new_data = None\n",
        "# predictions = sbs.predict(new_data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "d58456bdcf71a0aafcc61ee26750cc6fcc46b3f534d6a5983f0e2590bcc774fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
